{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI & Data Science Course 2023: Understanding and using LLMs\n",
    "-------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aims\n",
    "- To aid you in understanding how transformers work under the hood by peering into the components of a GPT-2 style transformer and building it from the ground up.\n",
    "    - Showcase libraries and tools along the way.\n",
    "- To demonstrate how you can leverage open source implementations rather than building them from scratch.\n",
    "    - Touch on a GPT-2 style use for text generation for the sake of similiarity but many others exist.\n",
    "\n",
    "### Prerequisites\n",
    "- No real hard prerequisites (can learn alot of it on the go!)\n",
    "- Having said that a few things would be *\"nice to have\"* to help absorb things faster\n",
    "    - Ideally some experience with programming (ideally Python)\n",
    "    - Some basic groundings in Math (Linear Algebra)\n",
    "    - Passion! Things evolve quickly and come with challenges, have to have determination to persevere.\n",
    " \n",
    "### Intro\n",
    "\n",
    "- Typically there are 3 ways people can interact with LLMs.\n",
    "    - Making your own from scratch (collecting data, deinfing architecture, training model,...)\n",
    "    - Making use of those made by others (either people or organisations)\n",
    "    - Hybrid of the two (e.g \"building ontop\" of a new model)\n",
    "- During our walkthrough we will explore the components (from scratch) and then end with making use of existing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using pre-existing models\n",
    "\n",
    "- HuggingFace can be thought of as a wide ecosystem which facilitates the open source nature of modern AI/ML\n",
    "    - Can do many things on huggingface but we will primarily touch on using their collections of models for tasks.\n",
    "- `Transformers` library in is part of this ecosystem and allows us access to all these models.\n",
    "- If you wanted you could also make use of there other libraries like `datasets/tokenizers` if you want to do some stuff on existing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with GPT 2\n",
    "\n",
    " - It is much easier working with GPT 2 out of the box compared to trying to build the model from scratch.\n",
    "   - In particular don't need to worry about troublesome training process\n",
    "- We make use of the HuggingFace ecosystem for this, here is the page for [GPT 2](https://huggingface.co/gpt2)\n",
    "- Various decoding generation strategies exist\n",
    "  - [Blog post](https://huggingface.co/blog/how-to-generate) on huggingface provides more details on how some of these work and can look at [docs](https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration) for using the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Nish and I like discussing AI and Data Science topics. I'm a software engineer and I'm currently\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\noutputs = gpt2_model.generate(**encoded_input,\\n                              do_sample=True, num_beams=1, max_new_tokens=10\\n                              )\\ngpt2_tokenizer.batch_decode(outputs, skip_special_tokens=True)\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"gpt2-medium\"\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "# gpt2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "input_text = ['My name is Nish and I like discussing AI and Data Science topics']\n",
    "\n",
    "encoded_input = gpt2_tokenizer(input_text, return_tensors='pt', truncation=True)\n",
    "\n",
    "output = gpt2_model.generate(**encoded_input,\n",
    "                            num_beams=1,\n",
    "                            max_new_tokens=10,\n",
    "                            # num_return_sequences=1,\n",
    "                            # top_k=50,\n",
    "                            # top_p=0.95,\n",
    "                            # temperature=0.7,\n",
    "                            do_sample=False\n",
    "                        )\n",
    "\n",
    "#Â prints out generated text to the console\n",
    "for generated_ids in output:\n",
    "    generated_text = gpt2_tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "outputs = gpt2_model.generate(**encoded_input,\n",
    "                              do_sample=True, num_beams=1, max_new_tokens=10\n",
    "                              )\n",
    "gpt2_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the components of LLMs\n",
    "\n",
    "- Here we will be looking at various components of the transformer and how you can go about implementing them.\n",
    "    - Particular focus is on GPT-2 based transformer architecture as touched on above though ideas are similiar throughout.\n",
    "- We will draw heavy inspiration from the code provided in this [notebook](https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb#scrollTo=EDlMEk0LVcdy) for implementing the granular code.\n",
    "    - The following [post](https://jalammar.github.io/illustrated-gpt2/) also provides a good breakdown of GPT-2.\n",
    "    - If interested I have written further about LLMs components in my [blog post](https://jalammar.github.io/illustrated-gpt2/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glossary\n",
    "\n",
    "##### Einops\n",
    "\n",
    "[Einops](https://einops.rocks/) is a great python library for performing tensor operations in a reliable way. It has good integration with a bunch of other deep learning frameworks.\n",
    "\n",
    "- Einops is great as it makes tensor manipulation much easier which typically can be error prone (at least for me...) for me this is largely due to it's interface.\n",
    "- Foundations come from einstein's summation notation (those you studied physics might have come across it before).\n",
    "- Here is a [great post](https://rockt.github.io/2018/04/30/einsum) which goes over the background theory.\n",
    "\n",
    "Related to this we make use of [fancy-einsum](https://pypi.org/project/fancy-einsum/) which provides slightly more convienient version of `torch.einsum` for our einstein summation calculation processes.\n",
    "\n",
    "\n",
    "##### Dataclasses\n",
    "\n",
    "Python's dataclasses module, introduced in Python 3.7, is a powerful tool for creating classes that primarily store data. Here's a summary of when to use it and how to apply it:\n",
    "\n",
    "- When to Consider Using dataclasses\n",
    "    - **Simplifying Class Definitions**: Use dataclasses when you need classes that mainly store data and you want to reduce boilerplate code. They're ideal for classes where you would traditionally write numerous __init__, __repr__, __eq__, and other dunder methods manually.\n",
    "    - **Immutable Data Structures**: If you need immutable data structures (similar to tuples), dataclasses with frozen parameters can be a good choice.\n",
    "    - **Comparing Object Instances**: They are useful when you need to compare instances based on their content rather than their identity in memory.\n",
    "    - **Lightweight Data Storage**: Ideal for classes that will be used to store data and not much else, especially when you need a clear and concise representation of the data structure.\n",
    "- How do use:\n",
    "    - **Decorate your class:** Use `@dataclass` around your class, can define data as class attributes.\n",
    "\n",
    "Example:\n",
    "```py\n",
    "@dataclass\n",
    "class MyClass:\n",
    "    field1: int\n",
    "    field2: str\n",
    "    field3: float = 0.0\n",
    "```\n",
    "\n",
    "##### Pytorch Primer\n",
    "\n",
    "For our purposes we can think of PyTorch as acting as the main library/framework which contains all the necessary tools for us to work with Deep Learning techniques (in particular building our model).\n",
    "\n",
    "PyTorch itself has so much information that it would be a standalone course in and of itself to get familiar with it. I highly suggest checking out the following [PyTorch Video course](https://youtu.be/V_xro1bcAuA?si=0eKJOeg86RGTCwMq) to get a solid foundational understanding.\n",
    "\n",
    "For our purposes I have added a few points below which showcases what things are.\n",
    "\n",
    "| PyTorch functionality | What does it do?|\n",
    "|---|---|\n",
    "|torch.nn\t| Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way).|\n",
    "|torch.nn.Parameter\t| Stores tensors that can be used with nn.Module. If requires_grad=True gradients (used for updating model parameters via gradient descent) are calculated automatically, this is often referred to as \"autograd\".|\n",
    "|torch.nn.Module | The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass nn.Module. Requires a forward() method be implemented.|\n",
    "|torch.optim\t| Contains various optimization algorithms (these tell the model parameters stored in nn.Parameter how to best change to improve gradient descent and in turn reduce the loss).|\n",
    "| def forward()\t| All nn.Module subclasses require a forward() method, this defines the computation that will take place on the data passed to the particular nn.Module (e.g. the linear regression formula above).|\n",
    "\n",
    "##### TransformerLens + CircuitViz\n",
    "\n",
    "- These libraries were designed specifically to aid the internal exploration of generative llms.\n",
    "    - [TransformerLens](https://github.com/neelnanda-io/TransformerLens): Enables the loading and working with open source models with cool utilities to enable more convenient internal exploration.\n",
    "    - [CirucitViz](https://github.com/alan-cooney/CircuitsVis): Enables nice visuals which work well inside a jupyter environment. \n",
    "- In this notebook transformer lens is used to load in a reference GPT-2 style model for comparison and allow the exploration of attention \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining important libraries\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import tqdm.auto as tqdm\n",
    "import circuitsvis as cv\n",
    "import transformer_lens\n",
    "from transformer_lens import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "# We make use of dataclasses so that we don't have to define a separate config when working inside a notebook\n",
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization Model Inputs\n",
    "\n",
    "- We have text data we want to feed into the model **_however_** the model doesn't like text but numbers. \n",
    "    - We therefore must convert our text to numbers via a process known as tokenization.\n",
    "- Many different types of tokenization exist and have various trade offs (Character based, word based, sub-word based)\n",
    "    - Short post touching on this [here](https://www.datacamp.com/blog/what-is-tokenization).\n",
    "    - **Character based**\n",
    "        - Split your text up into individual characters.\n",
    "        - Upsides in that you can handle mispelled words or rare words (impossible to not recognise something)\n",
    "        - Downsides are that you lose alot of semantic meaning behind your text since considering all text as just a stream of characters.\n",
    "            - This is bad since the objective behind *\"language modelling\"* is to extract meaning from your text.\n",
    "            - Would require vast resources (compute, memory, data etc) for this to even be viable.\n",
    "        - Rarely used in practice because of the downside.\n",
    "    - **Word based**\n",
    "        - Split your text into words.\n",
    "        - Upsides in that you don't need to force the model to try and learn words from characters\n",
    "            - Training process is slightly less complex\n",
    "        - Downsides:\n",
    "            - Punctuation and other rules may need to be added otherwise your punctuation would get merged with your words influencing learning.\n",
    "            - Things like word misspellings, conjugations, declinations and other grammatical things can cause a large vocab size\n",
    "            - Large vocab would then mean large compression task by the network having inefficiencies.\n",
    "                - Imagine a vocab of the order $10^5$ and model vectors of the order $10^3$ thats $10^8$ weights! Some models are that size alone\n",
    "            - Restricting vocab size can help overcome this however can cause information loss\n",
    "    - **Sub-word based**\n",
    "        - Splits your text into smaller \"subwords\" (slightly strange)\n",
    "        - Upside is that it balances the up and downsides of the prior techniques\n",
    "        - Various algorithms exist for this which similiar word based processes combine statistical and rule based algorithms. \n",
    "- Will demo below the general jist of tokenization below using a janky version of character based tokenization\n",
    "    - GPT-2 uses [byte-pair-encoding](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt) which is a form of sub-word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4,\n",
       " 23,\n",
       " 0,\n",
       " 17,\n",
       " 7,\n",
       " 16,\n",
       " 10,\n",
       " 0,\n",
       " 13,\n",
       " 20,\n",
       " 0,\n",
       " 5,\n",
       " 13,\n",
       " 20,\n",
       " 12,\n",
       " 0,\n",
       " 7,\n",
       " 17,\n",
       " 9,\n",
       " 0,\n",
       " 3,\n",
       " 0,\n",
       " 15,\n",
       " 13,\n",
       " 14,\n",
       " 10,\n",
       " 0,\n",
       " 9,\n",
       " 13,\n",
       " 20,\n",
       " 8,\n",
       " 22,\n",
       " 20,\n",
       " 20,\n",
       " 13,\n",
       " 17,\n",
       " 11,\n",
       " 0,\n",
       " 1,\n",
       " 3,\n",
       " 0,\n",
       " 7,\n",
       " 17,\n",
       " 9,\n",
       " 0,\n",
       " 2,\n",
       " 7,\n",
       " 21,\n",
       " 7,\n",
       " 0,\n",
       " 6,\n",
       " 8,\n",
       " 13,\n",
       " 10,\n",
       " 17,\n",
       " 8,\n",
       " 10,\n",
       " 0,\n",
       " 21,\n",
       " 18,\n",
       " 19,\n",
       " 13,\n",
       " 8,\n",
       " 20]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Character based \n",
    "corpus_text = 'My name is Nish and I like discussing AI and Data Science topics'\n",
    "character_split_corpus = list(corpus_text)\n",
    "token2id_map = {char: char_id for char_id, char in enumerate(sorted(set(character_split_corpus)))}\n",
    "\n",
    "tokenized_text = [token2id_map[token] for token in character_split_corpus]\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Embeddings\n",
    "\n",
    "- Remember our original input is text which we must convert to numbers via **_tokenization_** (outside the model)\n",
    "- We can then take these numbers and turn them into vectors (inside the model)\n",
    "    - These vectors are known as embeddings.\n",
    "- Different ways of implementing this which work out the same\n",
    "    - Can one-hot encode your input tokens and then multiply with the embedding matrix or index into it like done here.\n",
    "    - This indexing is more efficient and looks cleaner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        if self.cfg.debug: print(\"Tokens:\", tokens.shape)\n",
    "        embed = self.W_E[tokens, :] # [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Embeddings:\", embed.shape)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Embeddings\n",
    "\n",
    "- Interestingly the attention mechanism is symmetric about token position.\n",
    "    - No way of knowing that token 1 (source pos) comes prior to token 2 (source pos) relative to token 3 (dest pos)\n",
    "- This is problematic since attention itself moves information from source token positions to destination positions seemingly without knowing about position\n",
    "    - This fundamentally implies the process is flawed.\n",
    "- This is where positional embeddings come in!\n",
    "    - They provide a solution to this problem by *encoding* positional information about token positions into a vector format which can be added to the existing token embeddings.\n",
    "    - This process occurs prior to attention that way attention can use knowledge about position even if it doesn't do it itself!\n",
    "- Here we are allowing the model to learn the positional emedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        if self.cfg.debug: print(\"Tokens:\", tokens.shape)\n",
    "        pos_embed = self.W_pos[:tokens.size(1), :] # [position, d_model]\n",
    "        pos_embed = einops.repeat(pos_embed, \"position d_model -> batch position d_model\", batch=tokens.size(0))\n",
    "        if self.cfg.debug: print(\"pos_embed:\", pos_embed.shape)\n",
    "        return pos_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm\n",
    "\n",
    "- This normalises (or should I say standardises) the inputs so that they have a mean of 0 and variance of 1 and then scales and translates them\n",
    "    - Mathematically as:\n",
    "        -  **Compute Mean and Variance:**\n",
    "        $ \\mu = \\frac{1}{H} \\sum_{i=1}^{H} x_i \\quad$\n",
    "        $ \\sigma^2 = \\frac{1}{H} \\sum_{i=1}^{H} (x_i - \\mu)^2$\n",
    "        Here, $H$ is the number of hidden units.\n",
    "        - **Standardise:**\n",
    "        $ \\hat{x_i} = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} $\n",
    "        Where $\\epsilon$ is a small constant for numerical stability.\n",
    "        -  **Scale and Shift:**\n",
    "        $ y_i = \\gamma \\circ \\hat{x_i} + \\beta $\n",
    "        Here, $\\gamma$ and $\\beta$ are learnable parameters for scaling and shifting.\n",
    "        - **All in one:**\n",
    "        $Y = \\gamma \\left(\\frac{X - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\\right) + \\beta $\n",
    "    - This occurs independently and in parallel for each residual stream vector\n",
    "- This acts across the $d_{model}$ dimension and typically can occur prior or post the other layers (pre vs post layernorm)\n",
    "- Studies showed at the time it ensures a smoother process and greater accuracy in NLP tasks.\n",
    "    - This [stackpost](https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm) provides more details.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "    \n",
    "    def forward(self, residual):\n",
    "        # residual: [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Residual:\", residual.shape)\n",
    "        # Calculate the residual (x - mean)\n",
    "        residual = residual - einops.reduce(residual, \"batch position d_model -> batch position 1\", \"mean\")\n",
    "        # Calculate std by calculating the variance, square root it. Add in an epsilon to prevent divide by zero.\n",
    "        scale = (einops.reduce(residual.pow(2), \"batch position d_model -> batch position 1\", \"mean\") + cfg.layer_norm_eps).sqrt()\n",
    "        normalized = residual / scale\n",
    "        normalized = normalized * self.w + self.b\n",
    "        if self.cfg.debug: print(\"Normalized:\", residual.shape)\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "<img src=\"../data/images/attention_head.png\" alt=\"Attention Head\" width=\"400\"/>\n",
    "<img src=\"../data/images/multi_head_attention.png\" alt=\"Multi-Attention Heads\" width=\"400\"/>\n",
    "<img src=\"../data/images/causal_mask.png\" alt=\"Causal Mask\" width=\"400\"/>\n",
    "\n",
    "- What is attention? In particular the mechanism \n",
    "    - Remember the attention mechanism is all about learning efficient representations for your text.\n",
    "        - To do so it leverages the idea of *dot products* to create a similarity measure between your tokens $q \\times k^{T}$\n",
    "        - You can then generatate an *attention pattern/score* for each token (destination pos / query) which acts as a probability distribution over prior source tokens (keys).\n",
    "        - The values of the distribution then act as weights to decide on how much information to copy over \n",
    "        $\\text{softmax}(\\frac{q k^T}{\\sqrt{d_k}})$\n",
    "    - A another way of thinking about it is that attention is essentially *moving information between token positions* e.g from source positions (keys) to destination positions (queries)\n",
    "        - This moving in done in such a way to maximize the relevant information that is contained at each token position as per the relation between that token and all others that are *causally prior* to it in the case of GPT based models.\n",
    "    - This is the only part of the transformer which moves information between positions.\n",
    "- Why do you have multi-attention heads?\n",
    "    - Each head is meant to independently learn representations of your text (each has it's own set of parameters i.e weight matricies)\n",
    "    - You can then efficiently combine the knowledge learned by those heads to in theory gain a better understanding\n",
    "        - As the saying goes \"two-eyes are better than one\"\n",
    "    - Some cool maths can show that concatenating the heads outputs together is equivalent of linearly adding each output to the residual stream\n",
    "    - You generally find that the output dimension of the heads are smaller than the residual stream width e.g $\\frac{d_{model}}{d_{head}} = n_{heads}$\n",
    "- The way the outputs of each head are combined can be thought about differently\n",
    "    - Stacking the outputs of each head together and then performing a final linear map to get you back to the residual stream size is the same as multiplying each head by it's own weight matrix and then summing all the heads outputs together.\n",
    "    - Concatenation definition is often preferred since it produces a larger and more compute efficient matrix multiply but theoretically, they are equivalent and often preferred in a theoretical context to think of the heads as independently additive.\n",
    "- Thinking of it like this the operations can be formatted compactly into a single equation which yields the output of a single head: $ð´ ð¥ ð_{ð}^{ð} ð_{ð}^{ð}$\n",
    "    - $ð´$ is the attention matrix dimension ($ð_{ð}Ãð_{ð }$)\n",
    "        - Breaking down further as $qk^T$ where $q$ is our query vector and $k$ our key vector.\n",
    "        - Expanding this we get $q = xW_{Q}^{T}$ and $k = xW_{K}^{T}$ so neatly this becomes $A = xW_{Q}^{T} W_{K} x^{T}$\n",
    "    - $ð¥$ is the input matrix read from the residual stream consisting of your tokens and respective embedding ($ð_{ð}Ãð_{ððððð}$)\n",
    "    - $ð_{ð}^{ð}$ is the transpose of your value weight matrix ($ð_{ððððð}Ãð_{âððð}$)\n",
    "    - $ð_{ð}^{ð}$ is the transpose of your output weight matrix ($ð_{âððð}Ãð_{ððððð}$)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        \n",
    "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cpu\"))\n",
    "    \n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Normalized_resid_pre:\", normalized_resid_pre.shape)\n",
    "        \n",
    "        q = einsum(\"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\", normalized_resid_pre, self.W_Q) + self.b_Q\n",
    "        k = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_K) + self.b_K\n",
    "        \n",
    "        attn_scores = einsum(\"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\", q, k)\n",
    "        attn_scores = attn_scores / math.sqrt(self.cfg.d_head)\n",
    "        attn_scores = self.apply_causal_mask(attn_scores)\n",
    "\n",
    "        pattern = attn_scores.softmax(dim=-1) # [batch, n_head, query_pos, key_pos]\n",
    "\n",
    "        v = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_V) + self.b_V\n",
    "\n",
    "        z = einsum(\"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\", pattern, v)\n",
    "\n",
    "        attn_out = einsum(\"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\", z, self.W_O) + self.b_O\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(self, attn_scores):\n",
    "        # attn_scores: [batch, n_heads, query_pos, key_pos]\n",
    "        mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device), diagonal=1).bool()\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforword Network (MLP)\n",
    "\n",
    "<img src=\"../data/images/feedforward_layer.png\" alt=\"Feedforward (MLP) Layer\" width=\"400\"/>\n",
    "\n",
    "- This layer typically contains a single hidden layer\n",
    "    - Intuitively it's just a standard mlp layer which is meant to move information forward through the network\n",
    "- Mathematically it's just applying a linear map --> activation function --> linear map\n",
    "    - Activation function typically gelu for GPT based transformer\n",
    "- In my diagrams I refer to $d_{E} = d_{model}$ which is the residual stream size and in practice it's observed that $\\frac{d_{mlp}}{d_{model}} \\approx 4$\n",
    "    - Main thing to note the ratio is $\\geq 1$ which intuitively makes sense since you'd want it to create features and more them forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        self.gelu = nn.GELU()\n",
    "    \n",
    "    def forward(self, normalized_resid_mid):\n",
    "        # normalized_resid_mid: [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Normalized_resid_mid:\", normalized_resid_mid.shape)\n",
    "        pre = einsum(\"batch position d_model, d_model d_mlp -> batch position d_mlp\", normalized_resid_mid, self.W_in) + self.b_in\n",
    "        post = self.gelu(pre)\n",
    "        mlp_out = einsum(\"batch position d_mlp, d_mlp d_model -> batch position d_model\", post, self.W_out) + self.b_out\n",
    "        return mlp_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "\n",
    "<img src=\"../data/images/transformer_block.png\" alt=\"Showing the Transformer Block\" width=\"400\"/>\n",
    "\n",
    "- This packages together all the other components\n",
    "    - This main components of the block are sometimes referred to as *sub-layers* (attention and feedforward layers).\n",
    "- Typically if someone says \"this transformer has $N$ layers\" this means it has $N$ transformer block's and therefore \"$2N$ sub-layers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = FeedForwardLayer(cfg)\n",
    "    \n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre [batch, position, d_model]\n",
    "        normalized_resid_pre = self.ln1(resid_pre)\n",
    "        attn_out = self.attn(normalized_resid_pre)\n",
    "        resid_mid = resid_pre + attn_out\n",
    "        \n",
    "        normalized_resid_mid = self.ln2(resid_mid)\n",
    "        mlp_out = self.mlp(normalized_resid_mid)\n",
    "        resid_post = resid_mid + mlp_out\n",
    "        return resid_post"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un-embedding Layer\n",
    "\n",
    "- Just the final layer which maps you from your internal residual stream dimension back to the vocab dimension\n",
    "    - Remember the residual stream is just the internal dimension of the model\n",
    "- From this you can softmax over the vocab dimensional logits and subsequently sample from it giving you your generative abilities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
    "    \n",
    "    def forward(self, normalized_resid_final):\n",
    "        # normalized_resid_final [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Normalized_resid_final:\", normalized_resid_final.shape)\n",
    "        logits = einsum(\"batch position d_model, d_model d_vocab -> batch position d_vocab\", normalized_resid_final, self.W_U) + self.b_U\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-Transformer\n",
    "\n",
    "<img src=\"../data/images/gpt_style_transformer.png\" alt=\"Showing the full GPT-2 style transformer\" width=\"400\" height=\"400\"/>\n",
    "\n",
    "- Now you can combine everything together\n",
    "    - Can decide on how many blocks you want and then weave everything together in the order shown in the architecture diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2Model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens [batch, position]\n",
    "        embed = self.embed(tokens)\n",
    "        pos_embed = self.pos_embed(tokens)\n",
    "        residual = embed + pos_embed\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        normalized_resid_final = self.ln_final(residual)\n",
    "        logits = self.unembed(normalized_resid_final)\n",
    "        # logits have shape [batch, position, logits]\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = utils.get_device()\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (embed): Embed()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): FeedForwardLayer(\n",
       "        (gelu): GELU(approximate='none')\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm()\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference_model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\", device=device, fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "our_model = GPT2Model(Config(debug=False))\n",
    "# our_model.load_state_dict(reference_model.state_dict(), strict=False)\n",
    "#our_model = our_model.to(device)\n",
    "our_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Nish and I like discussing AI and Data Science topics'\n",
      "^^^Original Input^^^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|ââ        | 1/5 [00:01<00:05,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most probable token is  humorous and has probability 0.00018735218327492476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|ââââ      | 2/5 [00:02<00:04,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most probable token is  overhe and has probability 0.0002456027432344854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|ââââââ    | 3/5 [00:03<00:02,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most probable token is :{ and has probability 0.00017518943059258163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|ââââââââ  | 4/5 [00:04<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most probable token is flies and has probability 0.00014470017049461603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 5/5 [00:04<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most probable token is  humorous and has probability 0.00018375739455223083\n",
      "--------------------\n",
      "Generated text >>> \" humorous overhe:{flies humorous\"\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Our model\n",
    "test_string = \"My name is Nish and I like discussing AI and Data Science topics'\"\n",
    "print(test_string)\n",
    "print(\"^^^Original Input^^^\")\n",
    "generated_text = \"\"\n",
    "for i in tqdm.tqdm(range(10)):\n",
    "    # Tokenizing our text (turning it into a bunch of integers)\n",
    "    test_tokens = reference_model.to_tokens(test_string).cpu()\n",
    "    # Feeding in our tokens (getting out our logits)\n",
    "    demo_logits = our_model(test_tokens)\n",
    "    # Calculating our new token using greedy method\n",
    "    new_token = reference_model.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "    print(f\"The most probable token is {new_token} and has probability {demo_logits[-1, -1].softmax(dim=-1).max()}\")\n",
    "    test_string += new_token\n",
    "    generated_text += new_token\n",
    "print(\"--------------------\")\n",
    "print(f\"Generated text >>> \\\"{generated_text}\\\"\")\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Reference model\n",
    "test_string = \"My name is Nish and I like discussing AI and Data Science topics\"\n",
    "print(test_string)\n",
    "print(\"^^^Original Input^^^\")\n",
    "generated_text = \"\"\n",
    "for i in tqdm.tqdm(range(5)):\n",
    "    test_tokens = reference_model.to_tokens(test_string).cpu()\n",
    "    demo_logits = reference_model(test_tokens)\n",
    "    new_token = reference_model.tokenizer.decode(demo_logits[-1, -1].argmax())\n",
    "    print(f\"The most probable token is {new_token} and has probability {demo_logits[-1, -1].softmax(dim=-1).max()}\")\n",
    "    test_string += new_token\n",
    "    generated_text += new_token\n",
    "print(\"--------------------\")\n",
    "print(f\"Generated text >>> \\\"{generated_text}\\\"\")\n",
    "print(\"--------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention Pattern Visual\n",
    "\n",
    "- To aid in understanding attention it's useful to showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_string = \"\"\"My name is Nish and I like discussing AI and Data Science topics\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model_input_tokens = reference_model.to_tokens(model_input_string).cpu()\n",
    "print(model_input_tokens.device) # Should be on CPU\n",
    "reference_output_logits, reference_output_activations = reference_model.run_with_cache(model_input_tokens, remove_batch_dim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformer_lens.ActivationCache.ActivationCache'>\n",
      "torch.Size([12, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "print(type(reference_output_activations))\n",
    "attention_scores = reference_output_activations[\"pattern\", 0, \"attn\"]\n",
    "print(attention_scores.shape)\n",
    "model_str_tokens = reference_model.to_str_tokens(model_input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-5469f829-f512\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionHeads } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-5469f829-f512\",\n",
       "      AttentionHeads,\n",
       "      {\"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9376050233840942, 0.062394969165325165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7650384902954102, 0.1958978772163391, 0.03906362131237984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6739978194236755, 0.23332279920578003, 0.05668694153428078, 0.035992443561553955, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47700271010398865, 0.2329510748386383, 0.01623762957751751, 0.1825716644525528, 0.09123696386814117, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.45345503091812134, 0.20346498489379883, 0.07227212190628052, 0.04587305337190628, 0.1974649876356125, 0.027469754219055176, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4887450039386749, 0.15017051994800568, 0.03963890299201012, 0.025556670501828194, 0.2116735428571701, 0.02645059861242771, 0.057764794677495956, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3039196729660034, 0.17372873425483704, 0.03589732199907303, 0.07522352784872055, 0.15166185796260834, 0.04202679544687271, 0.16823524236679077, 0.049306854605674744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4494886100292206, 0.1388983279466629, 0.0422164648771286, 0.07041104882955551, 0.07997307926416397, 0.04926833137869835, 0.07296283543109894, 0.0520121268928051, 0.04476924240589142, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24214714765548706, 0.18905125558376312, 0.05600902438163757, 0.056668855249881744, 0.13780607283115387, 0.08672213554382324, 0.043522655963897705, 0.05838732793927193, 0.0703933909535408, 0.059292152523994446, 0.0, 0.0, 0.0, 0.0], [0.28125622868537903, 0.12223220616579056, 0.04702216759324074, 0.026341374963521957, 0.1274048089981079, 0.01461993157863617, 0.09897823631763458, 0.0554073229432106, 0.14165805280208588, 0.06781168282032013, 0.017267996445298195, 0.0, 0.0, 0.0], [0.22590509057044983, 0.10394638031721115, 0.014679289422929287, 0.021679945290088654, 0.16178038716316223, 0.03350140526890755, 0.03519589826464653, 0.028510600328445435, 0.03743946924805641, 0.24590617418289185, 0.038336560130119324, 0.05311884731054306, 0.0, 0.0], [0.19070753455162048, 0.07334541529417038, 0.05815628916025162, 0.02754025347530842, 0.08323603123426437, 0.06913414597511292, 0.0526924654841423, 0.14038820564746857, 0.034585945308208466, 0.05437156558036804, 0.07582502067089081, 0.07975923269987106, 0.06025785580277443, 0.0], [0.2656669616699219, 0.05057431757450104, 0.02687743306159973, 0.03489316627383232, 0.08755993098020554, 0.03365455940365791, 0.03320608288049698, 0.042667534202337265, 0.06424844264984131, 0.09159190207719803, 0.03487619757652283, 0.08742689341306686, 0.07695140689611435, 0.06980498880147934]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0001802251790650189, 0.999819815158844, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002493335341569036, 0.017839740961790085, 0.9819108843803406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001471947180107236, 0.009371188469231129, 0.0013485040981322527, 0.987808346748352, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [2.5217070287908427e-05, 0.0016474784351885319, 6.664468673989177e-05, 0.00016766285989433527, 0.9980929493904114, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0013790626544505358, 0.0061933365650475025, 0.0001566042483318597, 0.003992663696408272, 4.496454494073987e-05, 0.9882333874702454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0014373065205290914, 0.32613858580589294, 0.00028458182350732386, 0.0026392131112515926, 0.0011010915040969849, 0.002062110463157296, 0.6663370728492737, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0006494785193353891, 0.009343682788312435, 0.003033614717423916, 0.001208978472277522, 0.007177019026130438, 0.004363108891993761, 0.0016669795149937272, 0.9725571870803833, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00015434305532835424, 0.0024745515547692776, 0.00037198784411884844, 0.00020860698714386672, 0.0017690312815830112, 0.0004199375689495355, 0.00011211468518013135, 0.0002843675028998405, 0.9942051768302917, 0.0, 0.0, 0.0, 0.0, 0.0], [5.220413731876761e-05, 0.0008240947499871254, 9.991484694182873e-05, 0.00011869856098201126, 0.00016313880041707307, 4.7386438382091e-05, 0.00013577615027315915, 0.00010848809324670583, 4.5678654714720324e-05, 0.9984046816825867, 0.0, 0.0, 0.0, 0.0], [0.0006245642434805632, 0.0013565932167693973, 4.472249202081002e-05, 0.0014492084737867117, 1.2813648027076852e-05, 0.5120237469673157, 0.00046318030217662454, 0.004253392107784748, 5.2580111514544114e-05, 3.404196831979789e-05, 0.4796852469444275, 0.0, 0.0, 0.0], [6.169202970340848e-05, 0.00011723323405021802, 0.00031348210177384317, 0.0003811899805441499, 7.075275789247826e-06, 0.00020892028987873346, 3.250805093557574e-05, 5.701119880541228e-05, 6.346989539451897e-05, 0.00024370811297558248, 0.00013964182289782912, 0.9983741044998169, 0.0, 0.0], [0.0001229566551046446, 0.0038143943529576063, 0.0002495193330105394, 0.0003045746998395771, 0.00254997075535357, 0.0001127656432799995, 0.0003902735479641706, 0.0001395825674990192, 0.00023006121045909822, 0.0003828545450232923, 7.345978519879282e-05, 4.570758028421551e-05, 0.9915838241577148, 0.0], [0.00011268144589848816, 0.003104881150647998, 0.0008570468053221703, 0.00033952356898225844, 0.0031603858806192875, 8.738775795791298e-05, 0.00013038110046181828, 0.000283194676740095, 0.09554877132177353, 0.00011706148507073522, 6.277189822867513e-05, 0.0014362349174916744, 0.00223023840226233, 0.8925294876098633]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9610856175422668, 0.03891437128186226, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8714030981063843, 0.09219470620155334, 0.036402273923158646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.660275936126709, 0.12975940108299255, 0.09359326958656311, 0.11637144535779953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6401253342628479, 0.07588435709476471, 0.03472090885043144, 0.08920828998088837, 0.16006110608577728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36903536319732666, 0.06418900191783905, 0.08253724873065948, 0.05405993387103081, 0.02212762087583542, 0.4080508351325989, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.522541880607605, 0.09552702307701111, 0.04558340832591057, 0.13124296069145203, 0.05340101197361946, 0.08954115211963654, 0.0621626190841198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5049300193786621, 0.1118665486574173, 0.08914000540971756, 0.08518017083406448, 0.023726681247353554, 0.06460504978895187, 0.06055307015776634, 0.059998515993356705, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4825132191181183, 0.08793086558580399, 0.06561212986707687, 0.07760521024465561, 0.016348421573638916, 0.05370393395423889, 0.0468066968023777, 0.05988305062055588, 0.10959644615650177, 0.0, 0.0, 0.0, 0.0, 0.0], [0.43153607845306396, 0.06956393271684647, 0.05531580373644829, 0.04309997707605362, 0.18505628407001495, 0.02721952088177204, 0.04405926167964935, 0.03541557863354683, 0.03849949315190315, 0.07023414969444275, 0.0, 0.0, 0.0, 0.0], [0.18806707859039307, 0.03646945208311081, 0.04835967719554901, 0.033310674130916595, 0.014916646294295788, 0.2762947380542755, 0.019850509241223335, 0.05928118899464607, 0.014895855449140072, 0.01458401046693325, 0.29397013783454895, 0.0, 0.0, 0.0], [0.36539071798324585, 0.06520777940750122, 0.06841643154621124, 0.056598685681819916, 0.04316547513008118, 0.05227578058838844, 0.04391723498702049, 0.032637644559144974, 0.05739687383174896, 0.09418115764856339, 0.0486593022942543, 0.07215292006731033, 0.0, 0.0], [0.366173654794693, 0.04156205803155899, 0.059607915580272675, 0.052462272346019745, 0.19934765994548798, 0.030015835538506508, 0.04049253091216087, 0.0386846661567688, 0.044109221547842026, 0.005575723014771938, 0.028732948005199432, 0.03392519801855087, 0.05931035429239273, 0.0], [0.27806177735328674, 0.0417812205851078, 0.02262846753001213, 0.031716953963041306, 0.02278081700205803, 0.01913907751441002, 0.030077964067459106, 0.02082010544836521, 0.1320369988679886, 0.04299618676304817, 0.01793591119349003, 0.07388746738433838, 0.14575634896755219, 0.12038066983222961]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15053458511829376, 0.8494654297828674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02705349028110504, 0.004203620832413435, 0.968742847442627, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10858487337827682, 0.037151966243982315, 0.09947209060192108, 0.7547910809516907, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0018661236390471458, 9.770592441782355e-05, 3.879699943354353e-05, 8.109233749564737e-05, 0.9979162812232971, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07275208830833435, 0.017892468720674515, 0.03110477887094021, 0.06160226836800575, 0.01928904838860035, 0.7973593473434448, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002773919142782688, 0.008014041930437088, 0.0016895708395168185, 0.007287969812750816, 0.013709677383303642, 0.018269984051585197, 0.9482547640800476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004816895350813866, 0.0008924839785322547, 0.0017674241680651903, 0.0035919714719057083, 0.0015927430940791965, 0.011926906183362007, 0.1117946207523346, 0.8636168837547302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0012582535855472088, 9.182444773614407e-05, 0.0001105949777411297, 6.032628516550176e-05, 0.0005475005018524826, 0.00029863236704841256, 0.0004962747334502637, 0.00041423115180805326, 0.9967222809791565, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002801664813887328, 1.3960712749394588e-05, 4.89318736072164e-05, 1.1404108590795659e-05, 0.0009469534852541983, 4.499281203607097e-05, 0.0003705726412590593, 0.0009482869645580649, 0.002444054000079632, 0.994890570640564, 0.0, 0.0, 0.0, 0.0], [0.01149160135537386, 0.0006123428465798497, 0.0006633889279328287, 0.0013942290097475052, 0.0004602772824000567, 0.018523355945944786, 0.01035292074084282, 0.10568659752607346, 0.24498632550239563, 0.05843139812350273, 0.5473974943161011, 0.0, 0.0, 0.0], [0.02143044024705887, 0.00043549033580347896, 0.0003994035942014307, 0.00019785974291153252, 0.0014614238170906901, 0.00017532911442685872, 0.00164705456700176, 0.0011445527197793126, 0.013236191123723984, 0.0105464281514287, 0.0028878464363515377, 0.9464379549026489, 0.0, 0.0], [0.0015662347432225943, 1.5477704437216744e-05, 5.242322458798299e-06, 7.858109711378347e-06, 0.0002198995789512992, 4.186429578112438e-06, 6.702747486997396e-05, 8.216279093176126e-05, 0.0025360253639519215, 0.004098491743206978, 5.059171235188842e-05, 0.005141160450875759, 0.9862056374549866, 0.0], [0.003500391496345401, 1.3621285688714124e-05, 3.426305192988366e-05, 5.127983968122862e-06, 3.9516002289019525e-05, 7.078102498780936e-06, 4.399122917675413e-05, 8.597086707595736e-05, 0.008008323609828949, 0.00026758259627968073, 9.962984040612355e-05, 0.049616314470767975, 0.0061937556602060795, 0.932084321975708]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8759558796882629, 0.12404415011405945, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3627451956272125, 0.22273117303848267, 0.4145236313343048, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3620918393135071, 0.1801115721464157, 0.251380056142807, 0.2064165323972702, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01643061265349388, 0.0028488002717494965, 0.003437938168644905, 0.0024942955933511257, 0.9747883081436157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1084848940372467, 0.13906313478946686, 0.13511590659618378, 0.34674322605133057, 0.04238830879330635, 0.22820444405078888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21554939448833466, 0.12399494647979736, 0.041893452405929565, 0.09935630112886429, 0.1880238652229309, 0.13750608265399933, 0.19367602467536926, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1097622811794281, 0.05521085858345032, 0.05598118156194687, 0.10431234538555145, 0.0444268062710762, 0.10725118219852448, 0.12167976051568985, 0.4013754725456238, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04949641600251198, 0.015920596197247505, 0.022602904587984085, 0.011060911230742931, 0.027373524382710457, 0.028253188356757164, 0.013930419459939003, 0.050514671951532364, 0.7808473110198975, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01659850776195526, 0.00512502808123827, 0.0025460035540163517, 0.00414182199165225, 0.03823871538043022, 0.007159699220210314, 0.010501934215426445, 0.023285387083888054, 0.05068662762641907, 0.8417162299156189, 0.0, 0.0, 0.0, 0.0], [0.037178874015808105, 0.03238033875823021, 0.025534024462103844, 0.08608626574277878, 0.007185376714915037, 0.050307851284742355, 0.07294625788927078, 0.14627404510974884, 0.18999913334846497, 0.06271141767501831, 0.2893964350223541, 0.0, 0.0, 0.0], [0.05446143075823784, 0.008348808623850346, 0.0028373743407428265, 0.001498487894423306, 0.010177558287978172, 0.004561232402920723, 0.0035022813826799393, 0.004631259012967348, 0.016908524557948112, 0.018805228173732758, 0.01879274472594261, 0.8554750680923462, 0.0, 0.0], [0.01118641160428524, 0.0050774565897881985, 0.0011896856594830751, 0.0010937333572655916, 0.018602566793560982, 0.0016947021940723062, 0.004180784337222576, 0.00528318015858531, 0.027882568538188934, 0.13990505039691925, 0.006753446999937296, 0.11251600086688995, 0.6646345257759094, 0.0], [0.05484895408153534, 0.016309065744280815, 0.005807073786854744, 0.002217761939391494, 0.02567746676504612, 0.004343869164586067, 0.0038251520600169897, 0.007231901865452528, 0.02794617973268032, 0.017292656004428864, 0.015484744682908058, 0.14610838890075684, 0.10028696060180664, 0.5726197361946106]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1191956102848053, 0.8808043599128723, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11880417168140411, 0.0009591994457878172, 0.8802365660667419, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22307492792606354, 0.0010602101683616638, 0.005575336050242186, 0.7702895402908325, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0016906759701669216, 1.1905456631211564e-05, 5.244639282864227e-07, 1.66600067075251e-08, 0.9982969164848328, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.25583910942077637, 0.05864272639155388, 0.024639826267957687, 0.07804840058088303, 0.002986455336213112, 0.5798434615135193, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08573924750089645, 0.23750558495521545, 0.0010368035873398185, 0.00020027562277391553, 0.0004287900519557297, 0.0003070260281674564, 0.6747822761535645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05746029317378998, 0.011357927694916725, 0.005232043564319611, 0.00023723096819594502, 0.0009057726711034775, 0.0026160927955061197, 0.002757157664746046, 0.9194335341453552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0008578403503634036, 0.0001435351587133482, 4.729075044451747e-06, 1.038282334775431e-06, 0.0001601186377229169, 3.215022559288627e-07, 7.512471711379476e-07, 1.9859345457007294e-07, 0.9988313317298889, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0027328787837177515, 2.946743006759789e-05, 7.837978955649305e-06, 1.1097456535935635e-06, 1.2729335139738396e-05, 6.544353254867019e-07, 6.028299594618147e-06, 5.47950912732631e-06, 1.5720668670837767e-05, 0.9971880316734314, 0.0, 0.0, 0.0, 0.0], [0.08312298357486725, 0.0475054569542408, 0.019355135038495064, 0.07557978481054306, 0.001654975232668221, 0.39928653836250305, 0.015180915594100952, 0.038246624171733856, 0.012650392018258572, 0.002446934347972274, 0.3049702048301697, 0.0, 0.0, 0.0], [0.020963653922080994, 4.918102786177769e-05, 4.4389638787833974e-05, 2.417593350401148e-06, 9.368133646603383e-07, 4.592789537127828e-06, 1.572127075633034e-05, 1.4387682313099504e-05, 1.099305518437177e-05, 0.00012412655632942915, 2.0885129288217286e-06, 0.9787675142288208, 0.0, 0.0], [0.02346046455204487, 0.004425276070833206, 2.2360258299158886e-05, 1.417831936123548e-05, 2.937334284069948e-05, 1.0045279850601219e-05, 8.967403118731454e-05, 4.373250249045668e-06, 4.340378291090019e-05, 2.746424615907017e-05, 4.920547326037195e-06, 2.045915425696876e-05, 0.9718478918075562, 0.0], [0.007746630813926458, 0.00015951832756400108, 2.0220457372488454e-05, 5.664753643941367e-06, 0.00014846354315523058, 4.052149904509861e-07, 2.0329750896053156e-06, 1.7221701398284495e-07, 0.006398215424269438, 1.1550720046216156e-05, 1.5764550198582583e-07, 6.54877567285439e-06, 0.00013878488971386105, 0.9853616952896118]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9311129450798035, 0.06888701766729355, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7085821032524109, 0.13087250292301178, 0.16054539382457733, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4514380395412445, 0.23172004520893097, 0.26849284768104553, 0.04834916442632675, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3229314684867859, 0.13357006013393402, 0.14223811030387878, 0.1599443107843399, 0.2413160353899002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.322727233171463, 0.12718939781188965, 0.2025739550590515, 0.042982690036296844, 0.28592023253440857, 0.01860647276043892, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23372645676136017, 0.11405996233224869, 0.1210358515381813, 0.02789600007236004, 0.474685400724411, 0.008406340144574642, 0.020189939066767693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29810240864753723, 0.10912205278873444, 0.20727746188640594, 0.05574190989136696, 0.17264679074287415, 0.03776438534259796, 0.040883541107177734, 0.07846155017614365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3124138116836548, 0.06672307848930359, 0.10080470889806747, 0.027687925845384598, 0.20615901052951813, 0.044185154139995575, 0.028365429490804672, 0.032096706330776215, 0.18156416714191437, 0.0, 0.0, 0.0, 0.0, 0.0], [0.42817404866218567, 0.12216638028621674, 0.03889055177569389, 0.012289769016206264, 0.13041464984416962, 0.02343996800482273, 0.019043954089283943, 0.02563384734094143, 0.05724351853132248, 0.14270320534706116, 0.0, 0.0, 0.0, 0.0], [0.14233094453811646, 0.054371967911720276, 0.10436030477285385, 0.019089316949248314, 0.15729376673698425, 0.007852612063288689, 0.016809308901429176, 0.030997835099697113, 0.23200590908527374, 0.22590763866901398, 0.008980484679341316, 0.0, 0.0, 0.0], [0.3234170377254486, 0.11056192219257355, 0.033818501979112625, 0.007925658486783504, 0.10691941529512405, 0.015209808945655823, 0.015828371047973633, 0.012361095286905766, 0.056281525641679764, 0.11169301718473434, 0.014471364207565784, 0.19151219725608826, 0.0, 0.0], [0.2723020017147064, 0.0636436864733696, 0.05067894235253334, 0.01813567243516445, 0.06545285135507584, 0.018077583983540535, 0.019516080617904663, 0.01427711732685566, 0.04834425821900368, 0.08218640089035034, 0.01748584397137165, 0.18401163816452026, 0.1458880454301834, 0.0], [0.2729718089103699, 0.029164576902985573, 0.026810798794031143, 0.009621953591704369, 0.04652702063322067, 0.014139027334749699, 0.008043017238378525, 0.013198020868003368, 0.09807059913873672, 0.13399676978588104, 0.014216115698218346, 0.07384189963340759, 0.09845082461833954, 0.16094759106636047]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9837521314620972, 0.0162478219717741, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.45625069737434387, 0.46526408195495605, 0.07848524302244186, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.32185041904449463, 0.23979707062244415, 0.20657777786254883, 0.2317747324705124, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2783139646053314, 0.10244029760360718, 0.1046547219157219, 0.42017820477485657, 0.09441287070512772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14375831186771393, 0.09463460743427277, 0.09588124603033066, 0.14638987183570862, 0.21247419714927673, 0.3068616986274719, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12987518310546875, 0.08045432716608047, 0.03237580507993698, 0.10376740247011185, 0.06331198662519455, 0.3096539378166199, 0.28056129813194275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07869945466518402, 0.037504829466342926, 0.02540281042456627, 0.05567881837487221, 0.03065613843500614, 0.15855082869529724, 0.4406208395957947, 0.17288626730442047, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07947684079408646, 0.031616054475307465, 0.020983006805181503, 0.05760416388511658, 0.026150457561016083, 0.1461896449327469, 0.18357807397842407, 0.28454524278640747, 0.169856458902359, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09565939754247665, 0.02785004861652851, 0.009653356857597828, 0.055360980331897736, 0.05885419249534607, 0.11300896108150482, 0.12818074226379395, 0.1914210468530655, 0.1789027601480484, 0.14110852777957916, 0.0, 0.0, 0.0, 0.0], [0.040466051548719406, 0.012772705405950546, 0.010589136742055416, 0.017450401559472084, 0.021791847422719002, 0.035581476986408234, 0.09497898817062378, 0.06469441950321198, 0.16026979684829712, 0.23757518827915192, 0.30382996797561646, 0.0, 0.0, 0.0], [0.09649727493524551, 0.02105073817074299, 0.0063225263729691505, 0.028068024665117264, 0.041515547782182693, 0.0634184181690216, 0.07302727550268173, 0.07183447480201721, 0.0623854361474514, 0.023042229935526848, 0.4147968590259552, 0.09804119169712067, 0.0, 0.0], [0.007762168068438768, 0.004343709442764521, 0.001070195809006691, 0.002089390531182289, 0.0007235881639644504, 0.004108080640435219, 0.004607802722603083, 0.004458369221538305, 0.0052531748078763485, 0.022564416751265526, 0.025444703176617622, 0.9064515233039856, 0.011122812516987324, 0.0], [0.03820115327835083, 0.011328134685754776, 0.004050068557262421, 0.013349281623959541, 0.0038939323276281357, 0.0204701479524374, 0.022330569103360176, 0.030502133071422577, 0.13147956132888794, 0.01611156202852726, 0.11033463478088379, 0.2916114926338196, 0.19320723414421082, 0.11312994360923767]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9433581829071045, 0.0566418394446373, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7906429171562195, 0.10275428742170334, 0.10660286247730255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3058946430683136, 0.07719437777996063, 0.053201623260974884, 0.5637093186378479, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5590946674346924, 0.09880345314741135, 0.16621185839176178, 0.12875939905643463, 0.04713073745369911, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10334315896034241, 0.023138707503676414, 0.02317197620868683, 0.38586848974227905, 0.013652111403644085, 0.45082563161849976, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.192962184548378, 0.04069097712635994, 0.05157489329576492, 0.24698962271213531, 0.04053455963730812, 0.3178885877132416, 0.1093592494726181, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11740097403526306, 0.03088720515370369, 0.03975751996040344, 0.27540990710258484, 0.024545393884181976, 0.2773047089576721, 0.1457570344209671, 0.08893731981515884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29153501987457275, 0.062168292701244354, 0.08327385783195496, 0.14443552494049072, 0.058042071759700775, 0.14648427069187164, 0.08506160974502563, 0.11198914051055908, 0.017010308802127838, 0.0, 0.0, 0.0, 0.0, 0.0], [0.35386425256729126, 0.07860381156206131, 0.09423452615737915, 0.08035576343536377, 0.028566211462020874, 0.10199707746505737, 0.040305957198143005, 0.062258996069431305, 0.08405790477991104, 0.07575557380914688, 0.0, 0.0, 0.0, 0.0], [0.04582450911402702, 0.011300064623355865, 0.011503657326102257, 0.19664831459522247, 0.007156614679843187, 0.23901546001434326, 0.06888160854578018, 0.03879936411976814, 0.016372237354516983, 0.020702363923192024, 0.34379586577415466, 0.0, 0.0, 0.0], [0.27498647570610046, 0.09276183694601059, 0.0414595864713192, 0.0752146765589714, 0.040259938687086105, 0.07974845916032791, 0.06266044825315475, 0.07250378280878067, 0.06303707510232925, 0.06182742118835449, 0.08850228786468506, 0.047037962824106216, 0.0, 0.0], [0.27415958046913147, 0.06928696483373642, 0.05581323802471161, 0.08992110192775726, 0.03279213234782219, 0.07209637761116028, 0.06803397834300995, 0.03616306930780411, 0.05211254209280014, 0.04806879907846451, 0.07830940932035446, 0.07689790427684784, 0.04634491354227066, 0.0], [0.22689348459243774, 0.07709729671478271, 0.0800427570939064, 0.049474459141492844, 0.032585300505161285, 0.07263188064098358, 0.057617638260126114, 0.05193626135587692, 0.03480188176035881, 0.1006893739104271, 0.0810733437538147, 0.06349039822816849, 0.031197842210531235, 0.04046808183193207]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9149771928787231, 0.08502277731895447, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7882807850837708, 0.14309005439281464, 0.068629190325737, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6130199432373047, 0.14455051720142365, 0.10525143891572952, 0.13717813789844513, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6800978779792786, 0.133481964468956, 0.07475153356790543, 0.1027311459183693, 0.00893748365342617, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49548986554145813, 0.11048039048910141, 0.09528171271085739, 0.10859600454568863, 0.054686807096004486, 0.13546507060527802, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.448150634765625, 0.0895029827952385, 0.08328951150178909, 0.09775174409151077, 0.07375254482030869, 0.12401138246059418, 0.08354126662015915, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3748030364513397, 0.0896223708987236, 0.0903220996260643, 0.09856418520212173, 0.04846563935279846, 0.10957717150449753, 0.09284821152687073, 0.09579724818468094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38569945096969604, 0.10436802357435226, 0.06544242799282074, 0.08555471897125244, 0.060823097825050354, 0.08392048627138138, 0.08752190321683884, 0.08315020054578781, 0.0435197539627552, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4550119638442993, 0.09638610482215881, 0.05828896909952164, 0.06749357283115387, 0.0544700063765049, 0.07951254397630692, 0.06935090571641922, 0.06479113548994064, 0.044957004487514496, 0.009737752377986908, 0.0, 0.0, 0.0, 0.0], [0.30069100856781006, 0.0710798129439354, 0.06558875739574432, 0.07458234578371048, 0.04187776893377304, 0.08904911577701569, 0.07693000137805939, 0.08043582737445831, 0.05990438163280487, 0.045759256929159164, 0.09410178661346436, 0.0, 0.0, 0.0], [0.34357717633247375, 0.07058258354663849, 0.0716904029250145, 0.06356184929609299, 0.07933083176612854, 0.06703172624111176, 0.05575650557875633, 0.06320954114198685, 0.053249090909957886, 0.053085897117853165, 0.06978119164705276, 0.00914330780506134, 0.0, 0.0], [0.3748173415660858, 0.06543566286563873, 0.0500282384455204, 0.05738484114408493, 0.051414962857961655, 0.0701632872223854, 0.04980013146996498, 0.05652749538421631, 0.049358729273080826, 0.05541260167956352, 0.07218464463949203, 0.02929368056356907, 0.01817847229540348, 0.0], [0.27322155237197876, 0.06388350576162338, 0.05547335743904114, 0.06453925371170044, 0.045815981924533844, 0.060165174305438995, 0.05688847601413727, 0.05768004432320595, 0.04489678516983986, 0.05415330454707146, 0.06342699378728867, 0.04760100319981575, 0.06701606512069702, 0.04523847624659538]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6977688670158386, 0.302231103181839, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5685232281684875, 0.13723836839199066, 0.2942383587360382, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5037214756011963, 0.16115345060825348, 0.06567509472370148, 0.269449919462204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4619964361190796, 0.11905960738658905, 0.06645108014345169, 0.0825362503528595, 0.26995670795440674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4508134126663208, 0.15159307420253754, 0.07093345373868942, 0.14482074975967407, 0.03558794781565666, 0.1462513506412506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22669948637485504, 0.3103254437446594, 0.04647942632436752, 0.08459144830703735, 0.029212767258286476, 0.06172280013561249, 0.2409687042236328, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2892015874385834, 0.10982294380664825, 0.08603206276893616, 0.0917261615395546, 0.04694116488099098, 0.0921115130186081, 0.08635217696428299, 0.19781237840652466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.346293181180954, 0.08924634754657745, 0.04634036496281624, 0.08385603874921799, 0.04133157804608345, 0.08536821603775024, 0.06118052452802658, 0.037922680377960205, 0.20846104621887207, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2832273244857788, 0.1058889701962471, 0.046730075031518936, 0.07327094674110413, 0.03349442034959793, 0.0828971341252327, 0.0602702796459198, 0.04522727057337761, 0.03205133602023125, 0.23694218695163727, 0.0, 0.0, 0.0, 0.0], [0.23912924528121948, 0.08731723576784134, 0.04984750598669052, 0.11012224107980728, 0.036080654710531235, 0.12233796715736389, 0.08756822347640991, 0.06626290082931519, 0.04894151911139488, 0.04538872465491295, 0.10700386017560959, 0.0, 0.0, 0.0], [0.220147967338562, 0.06352179497480392, 0.05501042306423187, 0.07248904556035995, 0.01876429282128811, 0.08218816667795181, 0.04722290113568306, 0.030535303056240082, 0.031669046729803085, 0.051213089376688004, 0.06615162640810013, 0.2610863447189331, 0.0, 0.0], [0.24420179426670074, 0.07125524431467056, 0.025455687195062637, 0.05603964254260063, 0.04040773957967758, 0.061118919402360916, 0.049356646835803986, 0.025939999148249626, 0.038169074803590775, 0.032200947403907776, 0.04936883971095085, 0.023351281881332397, 0.2831341326236725, 0.0], [0.17639413475990295, 0.057428028434515, 0.03561427816748619, 0.055475857108831406, 0.030009489506483078, 0.052529122680425644, 0.039739273488521576, 0.025686992332339287, 0.12141929566860199, 0.028636859729886055, 0.046406686305999756, 0.03448381647467613, 0.05826486274600029, 0.237911194562912]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7942946553230286, 0.20570527017116547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.776637077331543, 0.1121167242527008, 0.1112462654709816, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6626712083816528, 0.11998660862445831, 0.08335326611995697, 0.13398896157741547, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34168630838394165, 0.1008262112736702, 0.10625182837247849, 0.300017774105072, 0.15121786296367645, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4549289047718048, 0.08376946300268173, 0.08441254496574402, 0.09494876861572266, 0.10597741603851318, 0.1759629249572754, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4594632387161255, 0.08876055479049683, 0.06672274321317673, 0.10964849591255188, 0.10208921879529953, 0.14152300357818604, 0.03179262951016426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.48812296986579895, 0.07257839292287827, 0.062330663204193115, 0.05252661556005478, 0.07781004160642624, 0.09082735329866409, 0.020632145926356316, 0.13517175614833832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.27104437351226807, 0.07447534799575806, 0.06257589161396027, 0.05791669338941574, 0.0746382549405098, 0.21910607814788818, 0.02221597172319889, 0.1561916172504425, 0.06183577701449394, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19750842452049255, 0.0736066922545433, 0.09619363397359848, 0.07298089563846588, 0.06337277591228485, 0.0988016426563263, 0.027240369468927383, 0.18772345781326294, 0.11058507114648819, 0.07198699563741684, 0.0, 0.0, 0.0, 0.0], [0.2966126799583435, 0.05248440057039261, 0.056910689920186996, 0.05914853885769844, 0.07674332708120346, 0.10014420747756958, 0.015706000849604607, 0.11815079301595688, 0.0731387585401535, 0.040955811738967896, 0.11000489443540573, 0.0, 0.0, 0.0], [0.2850993573665619, 0.04596405476331711, 0.07769589871168137, 0.08006369322538376, 0.06736240535974503, 0.06385503709316254, 0.02666493132710457, 0.10470876842737198, 0.08641932904720306, 0.03209610655903816, 0.06797051429748535, 0.06209998577833176, 0.0, 0.0], [0.18911050260066986, 0.06342087686061859, 0.06763659417629242, 0.07300462573766708, 0.0989779457449913, 0.04888812080025673, 0.028362084180116653, 0.07466321438550949, 0.09853069484233856, 0.04602089896798134, 0.053126197308301926, 0.07581084221601486, 0.08244743198156357, 0.0], [0.2279701828956604, 0.08129885792732239, 0.04952169582247734, 0.060354460030794144, 0.05577049031853676, 0.0682029202580452, 0.04866281896829605, 0.06784605234861374, 0.05180344730615616, 0.03336936607956886, 0.0709034651517868, 0.060726407915353775, 0.034755416214466095, 0.08881443738937378]]], \"negativeColor\": \"#FF0000\", \"positiveColor\": \"#00FF00\", \"tokens\": [\"<|endoftext|>\", \"My\", \" name\", \" is\", \" Nish\", \" and\", \" I\", \" like\", \" discussing\", \" AI\", \" and\", \" Data\", \" Science\", \" topics\"], \"maskUpperTri\": true}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x235ade54f10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Â We can use the attention scores to visualize the attention patterns for the first layer of the model\n",
    "cv.attention.attention_heads(tokens=model_str_tokens, attention=attention_scores, mask_upper_tri=True, negative_color=\"#FF0000\", positive_color=\"#00FF00\")\n",
    "# cv.attention.attention_patterns(tokens=model_str_tokens, attention=attention_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic training\n",
    "\n",
    "- Once you have defined your architecture you'll have to create your own custom training loop using PyTorch to train your model.\n",
    "    - Involves collecting data, training your model, testing it etc\n",
    "    - **Perhaps something we will touch on during another talk.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_cross_entropy_loss(logits, tokens):\n",
    "    # Measure next token loss\n",
    "    # Logits have shape [batch, position, d_vocab]\n",
    "    # Tokens have shape [batch, position]\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    pred_log_probs = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    return -pred_log_probs.mean()\n",
    "loss = lm_cross_entropy_loss(demo_logits, test_tokens)\n",
    "print(loss)\n",
    "print(\"Loss as average prob\", (-loss).exp())\n",
    "print(\"Loss as 'uniform over this many variables'\", (loss).exp())\n",
    "print(\"Uniform loss over the vocab\", math.log(our_model.cfg.d_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aidata_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
